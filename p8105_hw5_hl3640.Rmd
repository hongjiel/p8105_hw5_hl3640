---
title: "Homework 5"
author: "Hongjie Liu"
output: github_document
---


Load necessary packages for homework 5.

```{r loadpackages, message = FALSE}
library(tidyverse)
```


## Problem 1

Load and tidy dataset.

```{r p1_loadtidy, message = FALSE}
arm_df = list.files(path = "./data/", pattern = "^[ce]", full.names = T) %>% 
  map(~ read_csv(.x) %>% 
        mutate(arm = .x)) %>% 
  bind_rows %>% 
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  mutate(
    week = as.numeric(week),
    arm = str_extract(arm, "[a-z]{3}_[0-9]{2}")
  ) %>% 
  separate(arm, into = c("arm", "subject_id"), sep = "_") %>% 
  filter(arm == "con")
```

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r p1_plot}
arm_df %>% 
  ggplot(aes(x = week, y = observation, color = subject_id)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Observations on each subject over time",
    x = "Week",
    y = "Observation"
  )
```


## Problem 2

The data includes information of more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. The raw data includes:

* uid;
* reported date;
* basic demographic information about each victim, including first name, last name, race, age, and sex;
* the location of the killing, including state, city, longitude, and latitude;
* whether an arrest was made.

Create a city_state variable and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”). We assign the resulting dataset to `homicide_df`. We change one value of `city_state`, `Tulsa, AL`, to `Tulsa, OK`, because Tulsa is a city in Oklahoma, and the values of `uid`, `lat`, and `lon` indicate that the state should be `OK`.

```{r p2_readtidy, message = FALSE}
homicide_df =
  read_csv("./data/homicide-data.csv") %>% 
  unite(city_state, city:state, sep = ", ") %>% 
  mutate(city_state = replace(city_state, city_state == "Tulsa, AL", "Tulsa, OK")) %>% 
  group_by(city_state) %>% 
  summarize(
    n_obs = n(),
    n_unsolved_obs = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

homicide_df
```

For the city of Baltimore, MD, we use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of `prop.test` as an R object `unsolved_prop_bal`, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r prop_bal}
unsolved_prop_bal =
  prop.test(
    pull(filter(homicide_df, city_state == "Baltimore, MD"), n_unsolved_obs),
    pull(filter(homicide_df, city_state == "Baltimore, MD"), n_obs)
  )

unsolved_prop_bal %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

Now run `prop.test` for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. We create a tidy dataframe `unsolved_df` with estimated proportions and CIs for each city.

```{r prop_all}
unsolved_df =
  homicide_df %>% 
  mutate(
    htest = 
      map2(
        .x = n_unsolved_obs, .y = n_obs,
        ~ prop.test(x = .x, n = .y) %>% 
          broom::tidy()
      )
  ) %>% 
  unnest(htest) %>% 
  janitor::clean_names() %>% 
  select(city_state, estimate, conf_low, conf_high)

unsolved_df
```

Create a plot that shows the estimates and CIs for each city – check out `geom_errorbar` for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r p2_plot}
unsolved_df %>% 
  ggplot(aes(x = estimate, y = fct_reorder(city_state, estimate))) +
  geom_point() +
  geom_errorbar(aes(xmax = conf_high, xmin = conf_low)) +
  labs(
    title = "The estimates and CIs of the proportion of unsolved homicides",
    x = "Proportion of Unsolved Homicides",
    y = "City, State"
  )
```


## Problem 3

First set the following design elements:

* Fix $n = 30$
* Fix $\sigma = 5$

```{r p3_setvalues}
n = 30
sigma = 5
```

Set $\mu = 0$. Generate 5000 datasets from the model

$$ x \sim Normal[\mu, \sigma] $$

For each dataset, save $\hat{\mu}$ and the p-value arising from a test of $H : \mu = 0$ using $\alpha = 0.05$.

```{r p3_generate_dfs}
mu = 0
mu_null = 0
alpha = 0.05

set.seed(3640)

sim_results_df =
  rerun(
    5000,
    rnorm(n, mean = mu, sd = sigma) %>% 
      t.test(x = ., mu = mu_null) %>% 
      broom::tidy()
  ) %>% 
  bind_rows() %>% 
  select(estimate, p.value) %>% 
  mutate(mu = 0)

sim_results_df
```

Repeat the above for $\mu = \left\{1, 2, 3, 4, 5, 6\right\}$.

```{r p3_repeat}
for (i in 1:6) {
  
  sim_results_df_mu =
    rerun(
      5000,
      rnorm(n, mean = i, sd = sigma) %>% 
        t.test(x = ., mu = mu_null) %>% 
        broom::tidy()
    ) %>% 
    bind_rows() %>% 
    select(estimate, p.value) %>% 
    mutate(mu = i)
  
  sim_results_df = bind_rows(sim_results_df, sim_results_df_mu)
  
}

sim_results_df
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of $\mu$ on the x axis.

```{r p3_plot1}
sim_results_df %>% 
  group_by(mu) %>% 
  summarize(prop = sum(p.value < alpha) / n()) %>% 
  ggplot(aes(x = mu, y = prop)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True Mean",
    y = "Proportion of Times Rejecting The Null"
  )
```

Describe the association between effect size and power.

Make a plot showing the average estimate of $\hat{\mu}$ on the y axis and the true value of $\mu$ on the x axis.

```{r p3_plot_2}
sim_results_df %>% 
  group_by(mu) %>% 
  summarize(mean_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = mean_estimate)) +
  geom_point() +
  geom_line() +
  labs(
    title = "In All Samples",
    x = "True Mean",
    y = "Average Estimate of Mean"
  )
```

Make a second plot (or overlay on the first) the average estimate of $\hat{\mu}$ only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis.

```{r p3_plot_3}
sim_results_df %>% 
  filter(p.value < alpha) %>% 
  group_by(mu) %>% 
  summarize(mean_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = mean_estimate)) +
  geom_point() +
  geom_line() +
  labs(
    title = "In Samples for Which The Null Was Rejected",
    x = "True Mean",
    y = "Average Estimate of Mean"
  )
```

Is the sample average of $\hat{\mu}$ across tests for which the null is rejected approximately equal to the true value of $\mu$? Why or why not?