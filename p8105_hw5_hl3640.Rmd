---
title: "Homework 5"
author: "Hongjie Liu"
output: github_document
---


Load necessary packages for homework 5.

```{r loadpackages, message = FALSE}
library(tidyverse)
```


## Problem 1

Load and tidy dataset.

```{r p1_loadtidy, message = FALSE}
arm_df = list.files(path = "./data/", pattern = "^[ce]", full.names = T) %>% 
  map(~ read_csv(.x) %>% 
        mutate(arm = .x)) %>% 
  bind_rows %>% 
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  mutate(
    week = as.numeric(week),
    arm = str_extract(arm, "[a-z]{3}_[0-9]{2}")
  ) %>% 
  separate(arm, into = c("arm", "subject_id"), sep = "_")
```

Make a spaghetti plot showing observations on each subject over time.

```{r p1_plot}
arm_df %>% 
  ggplot(aes(x = week, y = observation, group = subject_id, color = arm)) +
  geom_path() +
  geom_point() +
  facet_grid(~arm) +
  labs(
    title = "Observations on each subject over time",
    x = "Week",
    y = "Observation"
  )
```

At the beginning of observations, subjects in the control arm and the experimental arm have roughly the same average of observation values. For the control arm, the observations of 10 subjects do not change generally over 8 weeks, while for the experimental arm, the observations of 10 subjects grow roughly linearly over 8 weeks.


## Problem 2

The data includes information of more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. The raw data includes:

* uid
* reported date
* basic demographic information about each victim, including first name, last name, race, age, and sex
* the location of the killing, including state, city, longitude, and latitude
* whether an arrest was made

We create a `city_state` variable and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides. We assign the resulting dataset to `homicide_df`. We change one value of `city_state`, `Tulsa, AL`, to `Tulsa, OK`, because Tulsa is a city in Oklahoma, and the values of `uid`, `lat`, and `lon` indicate that the state should be `OK`.

```{r p2_readtidy, message = FALSE}
homicide_df =
  read_csv("./data/homicide-data.csv") %>% 
  unite(city_state, city:state, sep = ", ") %>% 
  mutate(city_state = replace(city_state, city_state == "Tulsa, AL", "Tulsa, OK")) %>% 
  group_by(city_state) %>% 
  summarize(
    n_obs = n(),
    n_unsolved_obs = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

homicide_df
```

For the city of Baltimore, MD, we use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of `prop.test` as an R object `unsolved_prop_bal`, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r prop_bal}
unsolved_prop_bal =
  prop.test(
    pull(filter(homicide_df, city_state == "Baltimore, MD"), n_unsolved_obs),
    pull(filter(homicide_df, city_state == "Baltimore, MD"), n_obs)
  )

unsolved_prop_bal %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

Now we run `prop.test` for each of the cities in our dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. We create a tidy dataframe `unsolved_df` with estimated proportions and CIs for each city.

```{r prop_all}
unsolved_df =
  homicide_df %>% 
  mutate(
    htest = 
      map2(
        .x = n_unsolved_obs, .y = n_obs,
        ~ prop.test(x = .x, n = .y) %>% 
          broom::tidy()
      )
  ) %>% 
  unnest(htest) %>% 
  janitor::clean_names() %>% 
  select(city_state, estimate, conf_low, conf_high)

unsolved_df
```

We create a plot that shows the estimates and CIs for each city and organize cities according to the proportion of unsolved homicides.

```{r p2_plot, fig.width = 7.5, fig.height = 9}
unsolved_df %>% 
  ggplot(aes(x = estimate, y = fct_reorder(city_state, estimate))) +
  geom_point() +
  geom_errorbar(aes(xmax = conf_high, xmin = conf_low)) +
  labs(
    title = "Estimates and CIs of the proportion of unsolved homicides in 50 cities",
    x = "Proportion of Unsolved Homicides",
    y = "City, State"
  )
```


## Problem 3

First set the following design elements:

* Fix $n = 30$
* Fix $\sigma = 5$

```{r p3_setvalues}
n = 30
sigma = 5
```

Set $\mu = 0$. Generate 5000 datasets from the model

$$ x \sim Normal[\mu, \sigma] $$

For each dataset, save $\hat{\mu}$ and the p-value arising from a test of $H : \mu = 0$ using $\alpha = 0.05$.

```{r p3_generate_dfs}
mu = 0
mu_null = 0
alpha = 0.05

set.seed(3640)

sim_results_df =
  rerun(
    5000,
    rnorm(n, mean = mu, sd = sigma) %>% 
      t.test(x = ., mu = mu_null) %>% 
      broom::tidy()
  ) %>% 
  bind_rows() %>% 
  select(estimate, p.value) %>% 
  mutate(mu = 0)

sim_results_df
```

Repeat the above for $\mu = \{1, 2, 3, 4, 5, 6\}$.

```{r p3_repeat}
for (i in 1:6) {
  
  sim_results_df_mu =
    rerun(
      5000,
      rnorm(n, mean = i, sd = sigma) %>% 
        t.test(x = ., mu = mu_null) %>% 
        broom::tidy()
    ) %>% 
    bind_rows() %>% 
    select(estimate, p.value) %>% 
    mutate(mu = i)
  
  sim_results_df = bind_rows(sim_results_df, sim_results_df_mu)
  
}

sim_results_df
```

We make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of $\mu$ on the x axis.

```{r p3_plot1}
sim_results_df %>% 
  group_by(mu) %>% 
  summarize(prop = sum(p.value < alpha) / n()) %>% 
  ggplot(aes(x = mu, y = prop)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True Mean",
    y = "Proportion of Times Rejecting The Null"
  )
```

The power starts to increase sharply and then approaches the limit 1 as the true mean $\mu$ increases from 0.

We make a plot showing the average estimate of $\hat{\mu}$ on the y axis and the true value of $\mu$ on the x axis.

```{r p3_plot_2}
sim_results_df %>% 
  group_by(mu) %>% 
  summarize(mean_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = mean_estimate)) +
  geom_point() +
  geom_line() +
  labs(
    title = "In All Samples",
    x = "True Mean",
    y = "Average Estimate of Mean"
  )
```

We then make a second plot the average estimate of $\hat{\mu}$ only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis.

```{r p3_plot_3}
sim_results_df %>% 
  filter(p.value < alpha) %>% 
  group_by(mu) %>% 
  summarize(mean_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = mean_estimate)) +
  geom_point() +
  geom_line() +
  labs(
    title = "In Samples for Which The Null Was Rejected",
    x = "True Mean",
    y = "Average Estimate of Mean"
  )
```

The sample average of $\hat{\mu}$ across tests for which the null is rejected is approximately equal to the true value of $\mu$ when $\mu$ is zero and $\mu$ is large enough ($\mu = 4, 5, 6$ in our plots).

For every hypothesis test we conducted, the test statistic is $t = \displaystyle\frac{\hat{\mu} - 0}{s / \sqrt{n}}$, where $\hat{\mu}$ is the sample mean and $s$ is the sample standard deviation of the dataset we generated. The non-rejection region is $t \in (t_{29,0.025}, t_{29,0.975})$.

For every $\mu$, we have that $\displaystyle\frac{\hat{\mu} - \mu}{s / \sqrt{n}} \sim t_{29}$.

When $\mu = 0$, the distribution of the test statistic $t = \displaystyle\frac{\hat{\mu} - 0}{s / \sqrt{n}} \sim t_{29}$, and it is approximately symmetric about the mean 0. Since the rejection region is also symmetric about 0, the number of $\hat{\mu}$ that makes $t < t_{29,0.025}$ is approximately the same as the number of $\hat{\mu}$ that makes $t > t_{29,0.975}$, and the distribution of $\hat{\mu}$ that makes $t$ fall in the rejection region is approximately symmetric about 0. Thus, the sample average of $\hat{\mu}$ across tests for which the null is rejected is approximately equal to $\mu = 0$.

When $\mu$ is not large enough ($\mu = 1, 2, 3$), we have that $\displaystyle\frac{\hat{\mu} - \mu}{s / \sqrt{n}} \sim t_{29}$. Then for the test statistic $t = \displaystyle\frac{\hat{\mu} - 0}{s / \sqrt{n}}$, the number of $\hat{\mu} < \mu$ that makes $t$ fall in the non-rejection region is greater than the number of $\hat{\mu} > \mu$ that makes $t$ fall in the non-rejection region in almost all cases (we have 5,000 datasets), and the distribution of $\hat{\mu}$ is approximately symmetric about $\mu$. Also, since $\mu$ is close to 0, there are a certain amount of $\hat{\mu}$ that makes $t$ fall in the non-rejection region. Thus, the average estimate of $\hat{\mu}$ only in samples for which the null was not rejected is less than $\mu$ in almost all cases. In other words, the average estimate of $\hat{\mu}$ only in samples for which the null was rejected is greater than $\mu$ in almost all cases.

When $\mu$ is large enough ($\mu = 4, 5, 6$), we have that $\displaystyle\frac{\hat{\mu} - \mu}{s / \sqrt{n}} \sim t_{29}$, and it is a bell-shaped symmetric distribution. Then for the test statistic $t = \displaystyle\frac{\hat{\mu} - 0}{s / \sqrt{n}}$, since $\mu$ is far away from 0, there are hardly any $\hat{\mu}$ that makes $t$ fall in the non-rejection region. Thus, the average estimate of $\hat{\mu}$ only in samples for which the null was rejected is approximately equal to $\mu$.